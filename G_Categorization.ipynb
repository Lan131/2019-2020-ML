{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import h2o\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "h2o.init()\n",
    "from h2o.estimators.kmeans import H2OKMeansEstimator\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from h2o.transforms.decomposition import H2OPCA\n",
    "pd.options.display.max_columns= None\n",
    "from h2o.transforms.decomposition import H2OPCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from gensim.corpora.Dictionary import load_from_text, doc2bow\n",
    "#from gensim.corpora import MmCorpus\n",
    "#from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.remove_all()\n",
    "#try:\n",
    "#    h2o.cluster().shutdown()\n",
    "#except:\n",
    "#    print(\"Cluster was already closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont delete\n",
    "STOP_WORDS = [\"i\",\"you\",\"us\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\n",
    "               \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n",
    "               \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n",
    "               \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\n",
    "              \"like\",\"likes\",\"so\",\"(U)\",\"(U//FOUO)\",\"apos\",\"quot\",\"has\",\"been\",\"do\",\"had\",\"was\",\"were\",\n",
    "             \"they\",\"their\",\"them\",\"am\",\"will\",\"than\",\"when\",\"who\",\"where\",\"our\",\"me\",\"your\",\"would\",'don','get','more',\n",
    "             'nga','nro','nsa','new','adf','here','no','see','day','get','why','how','ve','dont','its','ive','work','people',\n",
    "              'out','which','should','ADF-C','leadership','ADFC']\n",
    "\n",
    "data=pd.read_excel(\"2016 ADF-C Climate Survey sample comments.xls\",encoding='utf-8')\n",
    "#data=pd.read_excel(\"sec_cat.xlsx\",sheetname=\"Sheet1\",encoding='utf-8')\n",
    "h2o_data=h2o.H2OFrame(pd.DataFrame(data['Final Comments']))\n",
    "h2o_data=h2o_data.ascharacter()\n",
    "h2o_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score=analyzer.polarity_scores(sentence)\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dat=data\n",
    "J=Dat.dropna()['Final Comments'].apply(str).apply(sentiment_analyzer_scores)\n",
    "J.to_frame(\"Sentiment\").to_csv(\"sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#break into sentenses and clean comments\n",
    "D=h2o_data.as_data_frame()\n",
    "D['Final Comments']=D['Final Comments'].apply(str)\n",
    "#D['Final Comments']\n",
    "#D=pd.DataFrame(D['Final Comments'].str.split('.', expand=True).values) # I have commented the breaking of sentenses for now\n",
    "H=D.stack(level=-1)\n",
    "H=H.to_frame()\n",
    "H[0]=H[0].str.replace('&;','')\n",
    "H[0]=H[0].str.replace('apos','')\n",
    "H[0]=H[0].str.replace(';','')\n",
    "H[0]=H[0].str.replace('&apos;','')\n",
    "H[0]=H[0].str.replace('quot','')\n",
    "H[0]=H[0].str.replace('&','')\n",
    "H[0]=H[0].str.replace('?','.')\n",
    "h2o_data=h2o.H2OFrame(H,column_names=['Final Comments'])\n",
    "h2o_data\n",
    "h2o_data=h2o_data[h2o_data['Final Comments'] != 'nan']\n",
    "h2o_data=h2o_data[h2o_data['Final Comments'] != 'None']\n",
    "h2o_data=h2o_data[h2o_data['Final Comments'] != '']\n",
    "h2o_data=h2o_data.ascharacter()\n",
    "#h2o_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_data.as_data_frame().to_csv(\"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This converts sentenses into vectors\n",
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "words=tokenize(h2o_data)\n",
    "print(\"Build word2vec model\")\n",
    "#gl=h2o.import_file('glove.6B.100d.txt')\n",
    "gl=h2o.import_file('glove.6B.300d.txt')\n",
    "w2v_model = H2OWord2vecEstimator(pre_trained=gl)\n",
    "w2v_model.train(training_frame=words)\n",
    "survey= w2v_model.transform(words, aggregate_method = \"AVERAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master=h2o_data.concat(survey)\n",
    "print(master.nrows)\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best K using BIC, this isn't needed but in practice I returned the same number of categories and is statistically more rigerous\n",
    "\n",
    "\n",
    "Best_BIC=100000\n",
    "BIC=-999999999999999\n",
    "K=1\n",
    "early_stop=int(master.nrows)\n",
    "while(K<early_stop):\n",
    "    K=K+1\n",
    "    kmeans_model=H2OKMeansEstimator(k=K,estimate_k=False,seed=1234)\n",
    "    m=kmeans_model.train(x=list(range(K)),training_frame=survey)\n",
    "    BIC=survey.nrows*np.log(kmeans_model.model_performance().totss()/survey.nrows)+K*np.log(survey.nrows)\n",
    "    print(BIC)\n",
    "    if Best_BIC>BIC:\n",
    "        Best_BIC=BIC\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can override K if we wish\n",
    "#K=9\n",
    "#kmeans_model=H2OKMeansEstimator(k=K,estimate_k=False,seed=1234)\n",
    "#m=kmeans_model.train(x=list(range(K)),training_frame=survey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master=kmeans_model.predict(survey).concat(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=master.as_data_frame()\n",
    "f.to_csv(\"Tagged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at single word frequencies\n",
    "Fre=pd.DataFrame()\n",
    "for i in range(int(master['predict'].max())+1):\n",
    "    words_nums=h2o.as_list(tokenize(master['Final Comments'][master['predict']==i,:]),use_pandas=True)['C1'].value_counts()\n",
    "    words_nums=pd.DataFrame(words_nums)\n",
    "    words_nums.index.name='Category '+str(i)+' Words'\n",
    "    words_nums.reset_index(inplace=True)\n",
    "    words_nums.rename(columns={'C1':'Category '+str(i)+' Frequencies'}, inplace=True)\n",
    "    Fre=pd.concat([Fre,words_nums],axis=1)\n",
    "Fre.to_csv(\"Word_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at bigrams\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "Fre_bigram=pd.DataFrame()\n",
    "Bigram_nums=pd.DataFrame()\n",
    "\n",
    "for i in range(int(master['predict'].max())+1):\n",
    "    Bigram_nums=pd.DataFrame()\n",
    "    words_nums=h2o.as_list(master['Final Comments'][master['predict']==i,:],use_pandas=True)\n",
    "    words_nums=pd.DataFrame(words_nums)\n",
    "    words_nums['bigrams']=words_nums['Final Comments'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
    "    Bigram_nums['bigrams']=words_nums['bigrams'].to_frame().stack(level=-1)\n",
    "    Bigram_nums=[w for w in Bigram_nums['bigrams'] if not w in STOP_WORDS]\n",
    "    Bigram_nums = [item for sublist in Bigram_nums for item in sublist]\n",
    "    Bigram_nums = [i for i in Bigram_nums if not i[0].lower() in STOP_WORDS and not i[1].lower() in STOP_WORDS ]\n",
    "    Bigram_nums = [i for i in Bigram_nums if not i[0].lower() in [''] and not i[1].lower() in [''] ]\n",
    "    Bigram_nums=pd.DataFrame(Bigram_nums,columns=['bigram_1','bigram_2'])\n",
    "    Bigram_nums['bigrams']=Bigram_nums['bigram_1']+\" \"+Bigram_nums['bigram_2']\n",
    "    Bigram_nums['bigrams']=Bigram_nums['bigrams'].str.strip()\n",
    "    Bigram_nums['bigrams']=Bigram_nums['bigrams'].str.replace('[^\\w\\s]','')\n",
    "    Bigram_nums['bigrams']=Bigram_nums['bigrams'].str.replace('\\d','')\n",
    "    Bigram_nums=Bigram_nums[\"bigrams\"].to_frame()\n",
    "    Bigram_nums=Bigram_nums[Bigram_nums[\"bigrams\"].str.split().apply(len)>1]\n",
    "    C=Bigram_nums.groupby(['bigrams']).bigrams.agg('count').to_frame()\n",
    "    C['Text']=C.index\n",
    "    C['Count']=C['bigrams']\n",
    "    J=C[['Text','Count']]\n",
    "    J.reset_index(inplace=True,drop=True)\n",
    "    J['Count']=J['Count'].apply(int)\n",
    "    J.sort_values(by=['Count'],ascending=False,inplace=True)\n",
    "    J.rename(columns={'Count':'Category '+str(i)+' Frequencies'}, inplace=True)\n",
    "    J.rename(columns={'Text':'Category '+str(i)+' Bigrams'}, inplace=True)\n",
    "    J.reset_index(inplace=True,drop=True)\n",
    "    Fre_bigram=pd.concat([Fre_bigram,J],axis=1)\n",
    "Fre_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function gloVe ($G$) takes a vector of comments from $\\alpha$ and maps them to a cartisian product $R^{n}$. Formally, $G: \\alpha^{n} \\rightarrow R^{n \\times n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_nums=h2o.as_list(tokenize(h2o_data),use_pandas=True)['C1'].value_counts()\n",
    "df=pd.DataFrame(words_nums)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate graphics\n",
    "\n",
    "PCA = H2OPCA(k=2,pca_method='Power')\n",
    "\n",
    "PCA.train(training_frame=master[:,2:102])\n",
    "\n",
    "master_Decomposed=PCA.predict(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_Decomposed=master_Decomposed.concat(master[:,0:2])\n",
    "master_Decomposed\n",
    "master_Decomposed.as_data_frame().to_csv(\"viz.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_recomposed=master_Decomposed.concat(master)\n",
    "df_mrc=master_recomposed.as_data_frame()\n",
    "f, ax= plt.subplots(figsize=(10,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\",data=df_mrc, hue='predict',palette=\"winter\" ,ax=ax, linewidth=0)\n",
    "plt.title('Comment Similarities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sns.set(style='darkgrid')\n",
    "    cat_0=df_mrc.query(\"predict == 0\")\n",
    "    cat_1=df_mrc.query(\"predict == 1\")\n",
    "    cat_2=df_mrc.query(\"predict == 2\")\n",
    "    cat_3=df_mrc.query(\"predict == 3\")\n",
    "    cat_4=df_mrc.query(\"predict == 4\")\n",
    "    cat_5=df_mrc.query(\"predict == 5\")\n",
    "    cat_6=df_mrc.query(\"predict == 6\")\n",
    "    f,ax = plt.subplots(figsize=(10,10))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.despine(f, left=True, bottom=True)\n",
    "\n",
    "    ax=sns.kdeplot(cat_0.PC1,cat_0.PC2,cmap=\"Reds\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_1.PC1,cat_1.PC2,cmap=\"Blues\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_2.PC1,cat_2.PC2,cmap=\"rainbow\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_3.PC1,cat_3.PC2,cmap=\"copper\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_4.PC1,cat_4.PC2,cmap=\"coolwarm\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_5.PC1,cat_5.PC2,cmap=\"Dark2\",shade=True,shade_lowest=False,legend=True)\n",
    "    ax=sns.kdeplot(cat_6.PC1,cat_6.PC2,cmap=\"viridis\",shade=True,shade_lowest=False,legend=True)\n",
    "\n",
    "    red=sns.color_palette(\"Reds\")[-2]\n",
    "    blue=sns.color_palette(\"Blues\")[-2]\n",
    "    rainbow=sns.color_palette(\"rainbow\")[-2]\n",
    "    copper=sns.color_palette(\"copper\")[-2]\n",
    "    coolwarm=sns.color_palette(\"coolwarm\")[-2]\n",
    "    Dark2=sns.color_palette(\"Dark2\")[-2]\n",
    "    viridis=sns.color_palette(\"viridis\")[-2]\n",
    "except:\n",
    "    print(\"Please check number of categories.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train supervised algorithm for determination as to which words link to which categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Custom K optimizer- Not for operation- please do not run below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best K using BIC\n",
    "\n",
    "def find_best_K():\n",
    "    Best_BIC=100000\n",
    "    BIC=0\n",
    "    K=1\n",
    "    early_stop=int(master.nrows)\n",
    "    while(K<early_stop):\n",
    "        K=K+1\n",
    "        kmeans_model=H2OKMeansEstimator(k=K,estimate_k=False,seed=1234)\n",
    "        m=kmeans_model.train(x=list(range(K)),training_frame=survey)\n",
    "        BIC=survey.nrows*np.log(kmeans_model.model_performance().totss()/survey.nrows)+K*np.log(survey.nrows)\n",
    "        print(BIC)\n",
    "        if Best_BIC>BIC:\n",
    "            Best_BIC=BIC\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "find_best_K()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F=between SSE /within SSE, higher is better\n",
    "F=2865/4655.00\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=int(np.ceil(np.sqrt(master.nrows/2.00)))\n",
    "kmeans_model=H2OKMeansEstimator(k=K,estimate_k=True,seed=1234)\n",
    "#kmeans_model=H2OKMeansEstimator(k=16,seed=1234)\n",
    "m=kmeans_model.train(x=list(range(K)),training_frame=survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F=20344/69907.00\n",
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
